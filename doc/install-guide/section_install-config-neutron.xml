<section xml:id="install-neutron"
    xmlns="http://docbook.org/ns/docbook"
    xmlns:xi="http://www.w3.org/2001/XInclude"
    xmlns:xlink="http://www.w3.org/1999/xlink"
    xmlns:svg="http://www.w3.org/2000/svg"
    xmlns:html="http://www.w3.org/1999/xhtml"
    version="5.0">
  <title>Installing and Configuring Openstack Networking</title>

  <section xml:id="common_setup">
    <title>PLACEHOLDER (IGNORE)</title>
    <para>PLACEHOLDER</para>
  </section>

  <section xml:id="install-neutron.dedicated-network-node">
    <title>Install Networking Services on the Network Node</title>

    <note>
      <para>Before we start, make sure your network node is set up according to <link linkend="common_setup">common setup</link> directions.</para>
    </note>

    <para>First, we must install the OpenStack Networking service on the node:</para>
    <screen os="ubuntu"><prompt>#</prompt> <userinput>sudo apt-get install neutron</userinput></screen>
    <screen os="rhel;centos;fedora"><prompt>#</prompt> <userinput>sudo yum install openstack-neutron</userinput></screen>
    <screen os="opensuse"><prompt>#</prompt> <userinput>zypper install openstack-neutron</userinput></screen>

    <para>Next, we must enable packet forwarding and disable packet destination filtering, so that the network node can coordinate traffic for the VMs.  We do this by editing the file <filename>/etc/sysctl.conf</filename></para>
    <programlisting language="ini">
      net.ipv4.ip_forward=1
      net.ipv4.conf.all.rp_filter=0
      net.ipv4.conf.default.rp_filter=0
    </programlisting>

    <note>
      <para>When dealing with system network-related configurations, it may be necessary to restart the network service to get them to take effect.  This can be done with the following command:</para>
      <screen os="ubuntu"><prompt>#</prompt> <userinput>sudo service networking restart</userinput></screen>
      <screen os="rhel;centos;fedora;opensuse"><prompt>#</prompt> <userinput>sudo service network restart</userinput></screen>
    </note>

    <para>Before continuing, we must create the required tenant and endpoint so that Neutron can interface with Keystone</para>
    <!-- TODO(sross): write this section -->

    <para>Now, we can install, and then configure, our networking plugin.  The networking plugin is what Neutron uses to perform the actual software-defined networking.  There are several options for this.  Choose one, follow the <link linkend="install-neutron.install-plugin">instructions</link> in the linked section, and then return here.</para>

    <para>Now that you've installed and configured a plugin (you did do that, right?), it is time to configure the main part of Neutron.  First, we configure Neutron core by editing <filename>/etc/neutron/neutron.conf</filename>:</para>
    <programlisting language="ini">
      auth_host = CONTROLLER_NODE_MGMT_IP
      admin_tenant_name = service
      admin_user = neutron
      admin_password = ADMIN_PASSWORD
      auth_url = http://CONTROLLER_NODE_MGMT_IP:25357/v2.0
      auth_strategy = keystone
      rpc_backend = YOUR_RPC_BACKEND
      PUT_YOUR_RPC_BACKEND_SETTINGS_HERE_TOO
    </programlisting>

    <para>Then, we just need to tell the DHCP agent how to actually handle the DHCP stuff.  Neutron has support for plugins for this purpose, but in general we just use the Dnsmasq plugin.  Edit <filename>/etc/neutron/dhcp_agent.ini</filename>:</para>
    <programlisting language="ini">
      dhcp_driver = neutron.agent.linux.dhcp.Dnsmasq
    </programlisting>

    <para>Now, restart the rest of Neutron:</para>
    <screen>
      <prompt>#</prompt> <userinput>service neutron-dhcp-agent restart</userinput>
      <prompt>#</prompt> <userinput>service neutron-l3-agent restart</userinput>
    </screen>

    <!-- TODO(sross): enable Neutron metadata as well? -->

    <para>Next, <link linkend="install-neutron.configure-networks">configure the base networks</link> and return here</para>

  </section>

  <section xml:id="install-neutron.install-plugin">
    <title>Installing and configuring the Neutron plugins</title>

    <section xml:id="install-neutron.install-plugin.ovs">
      <title>Installing the Open vSwitch (OVS) plugin</title>

      <para>First, we must install the Open vSwitch plugin and its dependencies</para>
      <screen os="rhel;fedora;centos"><prompt>#</prompt> <userinput>sudo yum install openstack-neutron-openvswitch</userinput></screen>
      <!-- TODO(sross): support other distros -->

      <para>Now, we start up Open vSwitch</para>
      <screen os="rhel;fedora;centos"><prompt>#</prompt> <userinput>sudo service openvswitch start</userinput></screen>

      <para>Next, we must do some initial configuration for Open vSwitch, no matter whether we are using VLANs or GRE tunneling.  We need to add the integration bridge (this connects to the VMs) and the external bridge (this connects to the outside world), called <literal>br-int</literal> and <literal>br-ex</literal>, respectively</para>
      <screen>
        <prompt>#</prompt> <userinput>ovs-vsctl add-br br-int</userinput>
        <prompt>#</prompt> <userinput>ovs-vsctl add-br br-ex</userinput>
      </screen>

      <para>Then, we add a "port" (connection) from the interface <replaceable>EXTERNAL_INTERFACE</replaceable> to br-ex</para>
      <screen><prompt>#</prompt> <userinput>ovs-vsctl add-port br-ex EXTERNAL_INTERFACE</userinput></screen>

      <para>In order for things to work correctly, we must also configure <replaceable>EXTERNAL_INTERFACE</replaceable> to not have an IP address and to be in promiscuous mode.  Additionally, we need to set the newly created <literal>br-ex</literal> interface to have the IP address that formerly belonged to <replaceable>EXTERNAL_INTERFACE</replaceable>.</para>
      <!-- TODO(sross): implement instructions for transfering IP and setting promisc mode -->

      <para>Finally, we can now configure the settings for the particular plugins.  First, there are some general <acronym>OVS</acronym> configuration options to set, no matter whether you use VLANs or GRE tunneling.  We need to tell L3 agent and DHCP agent we are using <acronym>OVS</acronym> by editing <filename>/etc/neutron/l3_agent.ini</filename> and <filename>/etc/neutron/dhcp_agent</filename> (respectively):</para>
      <programlisting language="ini">
        interface_driver = neutron.agent.linux.interface.OVSInterfaceDriver
      </programlisting>

      <para>Similarly, we need to also tell Neutron core to use <acronym>OVS</acronym> by editing <filename>/etc/neutron/neutron.conf</filename>:</para>
      <programlisting language="ini">
        core_plugin = neutron.plugins.openvswitch.ovs_neutron_plugin.OVSNeutronPluginV2
      </programlisting>

      <para>Finally, we need to tell the <acronym>OVS</acronym> plugin how to connect to the database by editing <filename>/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini</filename>:</para>
      <programlisting language="ini">
        [database]
        sql_connection = DATABASE_TYPE://neutron:NETURON_PASSWORD@CONTROLLER_NODE_HOSTNAME/neutron
      </programlisting>

      <para>Now, we must decide which networking type we want.  We can either use GRE tunneling or VLANs.  <link linkend="install-neutron.install-plugin.ovs.gre">GRE tunneling</link> can be easier and simpler to set up, but is less flexible in certain regards.  <link linkend="install-neutron.install-plugin.ovs.vlan">VLANs</link> are more flexible, but can be harder to set up and have more issues.</para>
      <!-- TODO(sross): support provider networks?  We need to modify things above for this to work -->
      <!-- TODO(sross): document firewall? -->

      <para>After having configured OVS, restart the <acronym>OVS</acronym> plugin:</para>
      <screen><prompt>#</prompt> <userinput>service neutron-openvswitch-agent restart</userinput></screen>

      <note>
        <para>Make sure to check that the plugin restarted successfully.  If you get errors about missing the file <filename>plugin.ini</filename>, simply make a symlink pointing at <filename>/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini</filename> with the "name" <filename>/etc/neutron/plugins.ini</filename></para>
      </note>

      <para>Now, return whence you came!</para>

      <section xml:id="install-neutron.install-plugin.ovs.gre">
        <title>Configuring the Neutron <acronym>OVS</acronym> plugin for GRE Tunneling</title>

        <para>First, we must configure the L3 agent and the DHCP agent to not use namespaces by editing <filename>/etc/neutron/l3_agent.ini</filename> and <filename>/etc/neutron/dhcp_agent.ini</filename> (respectively):</para>
        <programlisting language="ini">
          use_namespaces = False
        </programlisting>

        <para>Then, we tell the <acronym>OVS</acronym> plugin to use GRE tunneling, using an integration bridge of <literal>br-int</literal> and a tunneling bridge of <literal>br-tun</literal>, and to use a local IP of the whatever <replaceable>EXTERNAL_INTERFACE</replaceable>'s IP address was supposed to be.  Edit <filename>/etc/neutron/plugins/openvswitch/ovs_neutron_plugin.ini</filename>:</para>
        <programlisting language="ini">
          [ovs]
          tenant_network_type = gre
          tunnel_id_ranges = 1:1000
          enable_tunneling = True
          integration_bridge = br-int
          tunnel_bridge = br-tun
          local_ip = EXTERNAL_INTERFACE_IP
        </programlisting>

        <para>Now, return to the <acronym>OVS</acronym> general instruction</para>
      </section>

      <section xml:id="install-neutron.install-plugin.ovs.vlan">
          <title>Configuring the Neutron <acronym>OVS</acronym> plugin for VLANs</title>
          <para>TODO</para>
        <!-- TODO(sross): write up the VLAN instructions -->
      </section>
    </section>
  </section>

  <section xml:id="install-neutron.configure-networks">
    <title>Creating the base Neutron networks</title>

    <note>
      <para>In the upcoming sections, the text <replaceable>SPECIAL_OPTIONS</replaceable> may occur.  This should be replaced with any options specific to your networking plugin choices.  See <link linkend="install-neutron.configure-networks.plugin-specific">here</link> to check if your plugin needs any special options</para>
    </note>

    <para>First, we will create the external network, called <literal>ext-net</literal> (or something else, your choice).  This network represents a slice of the outside world.  VMs will not be directly linked to this network; instead, they will be on sub-networks and be assigned floating IPs from this network's subnet's pool of floating IPs.  Neutron will then route the traffic appropriately</para>
    <screen><prompt>#</prompt> <userinput>neutron net-create ext-net -- --router:external=True SPECIAL_OPTIONS</userinput></screen>

    <para>Next, we will create the associated subnet.  It should have the same gateway as <replaceable>EXTERNAL_INTERFAE</replaceable> would have had, and the same CIDR details as well.  It will not have DHCP, since it represents a slice of the external world:</para>
    <screen><prompt>#</prompt> <userinput>neutron subnet-create ext-net --allocation-pool start=FLOATING_IP_START,end=FLOATING_IP_END --gateway=EXTERNAL_INTERFACE_GATEWAY --enable_dhcp=False EXTERNAL_INTERFACE_CIDR</userinput></screen>

    <para>Now, create one or more initial tenants.  Choose one (we'll call it <replaceable>DEMO_TENANT</replaceable>) to use for the following parts</para>

    <para>Then, we will create the router attached to the external network.  This router will route traffic to the internal subnets as appropriate (you may wish to create it under the a given tenant, in which case you should append <literal>--tenant-id DEMO_TENANT_ID</literal> to the command)</para>
    <screen><prompt>#</prompt> <userinput>neutron router-create ext-to-int</userinput></screen>

    <para>Now, we'll connect the router to <literal>ext-net</literal> by setting the router's gateway as <literal>ext-net</literal>:</para>
    <screen><prompt>#</prompt> <userinput>neutron router-gateway-set EXT_TO_INT_ID EXT_NET_SUBNET_ID</userinput></screen>

    <para>Then, we'll create an internal network for <replaceable>DEMO_TENANT</replaceable> (and associated subnet over an arbitrary interal IP range, say, <literal>10.5.5.0/24</literal>), and connect it to the router by setting it as a port:</para>
    <screen>
      <prompt>#</prompt> <userinput>neutron net-create --tenant-id DEMO_TENANT_ID demo-net SPECIAL_OPTIONS</userinput>
      <prompt>#</prompt> <userinput>neutron subnet-create --tenant-id DEMO_TENANT demo-net 10.5.5.0/24 --gateway 10.5.5.1</userinput>
      <prompt>#</prompt> <userinput>neutron router-interface-add EXT_TO_INT_ID DEMO_NET_SUBNET_ID</userinput>
    </screen>

    <para>Now, check your plugin's special options page to see if there are steps left to perform, and then return whence you came</para>

    <section xml:id="install-neutron.configure-networks.plugin-specific">
      <title>Plugin-specific Neutron networks options</title>
      <section xml:id="install-neutron.configure-networks.plugin-specific.ovs">
        <title>Open vSwitch Network Configuration Options</title>
        <section xml:id="install-neutron.configure-networks.plugin-specific.ovs.gre">
          <title>GRE Tunneling network options</title>
          <para>When creating networks, you should use the options</para>
          <screen><userinput> --provider:network_type gre --provider:segmentation_id SEG_ID</userinput></screen>

          <para><replaceable>SEG_ID</replaceable>should be <literal>1</literal> for the external network, and just an incremented number for other networks (it should be unique)</para>

          <para>After you have finshed creating all the networks, we need to specify which some more details for the L3 agent.  We need to tell it what the external network's id is, as well as the id of the router associated with this machine (because we are not using namespaces, there can be only one router per machine).  To do this, edit <filename>/etc/neutron/l3_agent.ini</filename>:</para>
          <programlisting language="ini">
            gateway_external_network_id = EXT_NET_ID
            router_id = EXT_TO_INT_ID
          </programlisting>

          <para>Then, restart the L3 agent</para>
          <screen><prompt>#</prompt> <userinput>service neutron-l3-agent restart</userinput></screen>

          <para>and return whence you came!</para>
        </section>
        <section xml:id="install-neutron.configure-networks.plugin-specific.ovs.vlan">
          <title>VLAN network options</title>
          <para>TODO</para>
          <!-- TODO(sross): fill this in -->
        </section>
      </section>
    </section>
  </section>

  <section xml:id="install-neutron.dedicated-compute-node">
    <title>Install Required Networking Support on a Dedicated Compute Node</title>

    <note>
      <para>This is for any node which is running compute services but is not running the full network stack</para>
    </note>

    <note>
      <para>Before we start, make sure your compute node is set up according to <link linkend="common_setup">common setup</link> directions.</para>
    </note>

    <para>To start out, we need to disable packet destination filtering (route verification) in order to let the networking services route traffic to the VMs.  Edit <filename>/etc/sysctl.conf</filename> (and then restart networking):</para>
    <programlisting language="ini">
      net.ipv4.conf.all.rp_filter=0
      net.ipv4.conf.default.rp_filter=0
    </programlisting>

    <para>Next, we need to install and configure plugin components.  Follow the <link linkend="install-neutron.install-plugin">instructions</link> for configuring and installing <emphasis>only</emphasis> the plugin and neutron core, <emphasis>not</emphasis> the L3 or DHCP agents, or network interface modification. Return here, and then, restart the Neutron plugin services.</para>

    <para>Now that you've installed and configured a plugin (you did do that, right?), it is time to configure the main part of Neutron by editing <filename>/etc/neutron/neutron.conf</filename>:</para>
    <programlisting language="ini">
      auth_host = CONTROLLER_NODE_MGMT_IP
      admin_tenant_name = service
      admin_user = neutron
      admin_password = ADMIN_PASSWORD
      auth_url = http://CONTROLLER_NODE_MGMT_IP:25357/v2.0
      auth_strategy = keystone
      rpc_backend = YOUR_RPC_BACKEND
      PUT_YOUR_RPC_BACKEND_SETTINGS_HERE_TOO
    </programlisting>
  </section>
</section>
